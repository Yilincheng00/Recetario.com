<head>
  <title>RECETARIO.com</title>
  
<style>
body {
      background-image: url('https://png.pngtree.com/thumb_back/fh260/background/20230611/pngtree-headphones-on-a-lake-at-sunset-image_2966375.jpg');

      background-size: cover;
 background-color: green;
      background-repeat: no-repeat;
      background-attachment: fixed;
    } 
     .post {
            background-color: #fff;
            padding: 20px;
            margin-bottom: 20px;
            border: 2px solid #ccc;
        }
  
</style>
  
</head>

<body>
  
   <div class="post">
  <h1>PhotoGuard</h1>  
       <nav class="menu">
  <ul>
    <li><a href="index.html">Inicio</a></li>
</nav>
  </div>


 <div class="post">
</br>
 <img src="https://play-lh.googleusercontent.com/Kpwpw6mB0gR5H62UBTyXw2x7zZ5vrT1wshaMzVRQVRe4cHCkluyHkP6gQIMBBuJUM3s=w240-h480-rw" width="300" height="300">
  </div>


 <div class="post">
<h2>¿Qué es?</h2>
   El MIT, ha desarrollado un sistema llamado Photoguard que pone dificultades a los modelos de IA a la hora de trabajar para que no puedan utilizar fotografías como base. Ha sido probada con herramientas como Stable Diffusion, cuyos resultados, en condiciones normales, son muy realistas.
   </br>
  </br>
"PhotoGuard", evita la manipulación no autorizada de imágenes, salvaguardando la autenticidad en la era de los modelos generativos avanzados.
  <br/>
  <br/>
 </div>

 <div class="post">
<h2>¿Cómo funciona?</h2>
  Al pasar las imágenes por esta herramienta, busca una perturbación en los datos que pueda hacer más grande para engañar al modelo de IA y que este no analice la información correctamente. Tras esto, se añaden estas modificaciones a los datos originales para reemplazar los reales.
   </br>
  </br>
Esto supone una barrera de protección para evitar los deepfakes, que suplantan el rostro de personas sin su consentimiento y que en muchas ocasiones pueden violar el derecho a la propia imagen de las personas afectadas por estos montajes. De hecho, las noticias falsas en las que se utilizaban ilustraciones falsas fueron uno de los motivos por los que el modelo gratuito de Midjourney se cerró.
  <br/>
  <br/>
 </div>

 <div class="post">
<h2>Métodos de Proteccion de PhotoGuard</h2>
    <img src="https://news.mit.edu/sites/default/files/styles/news_article__image_gallery/public/images/202307/malicious-ai-image.png?itok=LR9lmlf-" width="600" height="400">
    </br>
   </br>
   
 PhotoGuard utiliza dos métodos de "ataque" diferentes para generar estas perturbaciones:
   </br>
  </br>
El ataque de "codificador" más sencillo tiene como objetivo la representación latente de la imagen en el modelo de IA, lo que hace que el modelo perciba la imagen como una entidad aleatoria.
</br>
El ataque del codificador introduce ajustes menores en esta representación matemática, lo que hace que el modelo de IA perciba la imagen como una entidad aleatoria. Como resultado, cualquier intento de manipular la imagen utilizando el modelo se vuelve casi imposible. Los cambios introducidos son tan mínimos que resultan invisibles al ojo humano, preservando así la integridad visual de la imagen garantizando al mismo tiempo su protección.
</br>
  <br/>
  <br/>
La "difusión" más sofisticada define una imagen objetivo y optimiza las perturbaciones para hacer que la imagen final se parezca lo más posible al objetivo.
</br>
El ataque de “difusión”, decididamente más complejo, apunta estratégicamente a todo el modelo de difusión de un extremo a otro. Esto implica determinar una imagen objetivo deseada y luego iniciar un proceso de optimización con la intención de alinear estrechamente la imagen generada con este objetivo preseleccionado.

   
  <style> table{ border-collapse: collapse; } td{ border:1px solid #000; padding:25px; background-color:#B2FFFF; } </style>
   </a>
</div>

</body>
